{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üöÄ Step 1: Install Dependencies\n",
    "\n",
    "This cell installs all the necessary Python packages for our indexing process.\n",
    "- We upgrade `pip` and `numpy` first to ensure compatibility and avoid dependency conflicts within the Colab environment.\n",
    "- Then, we install all required `langchain` components, `chromadb` for the vector store, `sentence-transformers` for the embedding model, and other utilities.\n",
    "\n",
    "After the installation is complete, the Colab session (runtime) is automatically restarted. This is a crucial step to ensure that all newly installed libraries are correctly loaded into the environment. You might see a \"Session crashed\" warning, which is expected and normal behavior."
   ],
   "id": "8af84b9685dbff4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Update pip and install all required packages.\n",
    "# The `-q` flag stands for \"quiet\" to reduce installation log verbosity.\n",
    "!pip install --upgrade -q pip\n",
    "!pip install --upgrade -q numpy\n",
    "!pip install -q langchain langchain-core langchain-community langchain-chroma langchain-huggingface langchain-openai sentence-transformers tqdm\n",
    "\n",
    "# Restart the runtime automatically to apply changes.\n",
    "# This is a standard procedure in Colab after installing major packages.\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ],
   "id": "58fbb30703fabb90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìÇ Step 2: Connect to Google Drive\n",
    "\n",
    "This cell mounts your personal Google Drive to the Colab environment, allowing us to access files stored there.\n",
    "\n",
    "When you run this cell, you will be prompted to authorize access.\n",
    "1. Click the authorization link.\n",
    "2. Sign in to your Google account.\n",
    "3. Grant permission to Google Colab.\n",
    "4. Copy the authorization code provided and paste it into the input box in this cell.\n",
    "\n",
    "Once mounted, your entire Google Drive will be accessible under the directory `/content/drive/MyDrive/`. This allows us to read our source data (`content.jsonl`) and, more importantly, to save the persistent vector database directly to your Drive."
   ],
   "id": "559d10f874b75ceb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "except ImportError:\n",
    "    # This library is only available in the Google Colab environment.\n",
    "    # We can pass silently when running locally.\n",
    "    drive = None\n",
    "\n",
    "# We only attempt to mount if the 'drive' object was successfully imported.\n",
    "if drive:\n",
    "    # Mount Google Drive to the Colab virtual machine.\n",
    "    # The '/content/drive' directory will serve as the mounting point.\n",
    "    drive.mount('/content/drive')"
   ],
   "id": "b247ebd690d3267c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ‚öôÔ∏è Step 3: Configure, Process, and Index the Data\n",
    "\n",
    "This is the main part of our notebook. It performs the entire RAG indexing pipeline from start to finish.\n",
    "\n",
    "### The process is as follows:\n",
    "1.  **Configuration**: We define all necessary parameters, such as file paths on Google Drive, the name of the embedding model, and text splitting settings. **Please verify that `PROJECT_DIR_COLAB` matches the name of your folder on Google Drive.**\n",
    "2.  **Load & Prepare Data**: The script reads the `content.jsonl` file, extracts relevant text and metadata (`url`, `title`), and formats it into a standardized `Document` object.\n",
    "3.  **Split Documents**: Long documents are split into smaller, overlapping chunks. This is crucial for providing focused context to the language model later on.\n",
    "4.  **Check Existing Index**: The script checks if a vector database already exists and what documents are already indexed. This makes the process resumable ‚Äì if it's interrupted, you won't lose your progress.\n",
    "5.  **Generate Embeddings & Index**: For any new documents, the script uses a powerful multilingual model to generate vector embeddings. These embeddings, along with the document chunks and metadata, are then stored in a ChromaDB vector database on your Google Drive. This is the most computationally intensive step, which is why we are using a GPU.\n",
    "\n",
    "When you run this cell, you will see a progress bar (`tqdm`) showing the indexing process."
   ],
   "id": "7f0b3ae807bd2b69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: MAIN INDEXING SCRIPT FOR GOOGLE COLAB\n",
    "# ==============================================================================\n",
    "import json\n",
    "import logging\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# --- Part 1: Imports ---\n",
    "# Import all libraries installed in Cell 1\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging to display informative messages\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# --- Part 2: Configuration ---\n",
    "# PLEASE VERIFY THIS PATH MATCHES YOUR GOOGLE DRIVE FOLDER\n",
    "PROJECT_DIR_COLAB = Path(\"/content/drive/MyDrive/AkademikAI_Colab/\")\n",
    "\n",
    "class IndexingSettings:\n",
    "    \"\"\"A simple class to hold configuration parameters.\"\"\"\n",
    "    DATA_PATH: Path = PROJECT_DIR_COLAB / \"content.jsonl\"\n",
    "    DB_PATH: Path = PROJECT_DIR_COLAB / \"vector_db\"\n",
    "    EMBEDDING_MODEL_NAME: str = 'intfloat/multilingual-e5-large'\n",
    "    CHUNK_SIZE: int = 1500  # Larger chunk size can be effective for overview context\n",
    "    CHUNK_OVERLAP: int = 200\n",
    "\n",
    "settings = IndexingSettings()\n",
    "\n",
    "\n",
    "# --- Part 3: Helper Functions ---\n",
    "\n",
    "def load_and_prepare_documents(file_path: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads data from a .jsonl file and transforms it into a list of\n",
    "    LangChain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # We enrich the main text with title and H1 for better semantic context\n",
    "            enriched_content = f\"Page Title: {data.get('title', '')}\\n\" \\\n",
    "                               f\"H1 Header: {data.get('h1', '')}\\n\\n\" \\\n",
    "                               f\"{data.get('text', '')}\"\n",
    "            metadata = {\n",
    "                \"source\": data.get('url', ''),\n",
    "                \"title\": data.get('title', ''),\n",
    "            }\n",
    "            doc = Document(page_content=enriched_content, metadata=metadata)\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "def create_chunk_id(chunk: Document, chunk_index: int) -> str:\n",
    "    \"\"\"\n",
    "    Creates a unique and deterministic ID for a document chunk\n",
    "    by hashing its content and its index.\n",
    "    \"\"\"\n",
    "    unique_string = f\"{chunk.metadata.get('source', '')}{chunk.page_content}{chunk_index}\"\n",
    "    return hashlib.sha256(unique_string.encode()).hexdigest()\n",
    "\n",
    "# --- Part 4: Main Indexing Logic ---\n",
    "\n",
    "def run_indexing():\n",
    "    \"\"\"Main function to create and populate the vector database.\"\"\"\n",
    "    logging.info(\"üöÄ Starting the indexing process in Google Colab...\")\n",
    "\n",
    "    logging.info(f\"üìë Loading documents from: {settings.DATA_PATH}\")\n",
    "    if not settings.DATA_PATH.exists():\n",
    "        logging.error(f\"Data file not found! Please check the path: {settings.DATA_PATH}\")\n",
    "        return\n",
    "    documents = load_and_prepare_documents(settings.DATA_PATH)\n",
    "\n",
    "    logging.info(f\"üî™ Splitting {len(documents)} documents into chunks (chunk_size={settings.CHUNK_SIZE})...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=settings.CHUNK_SIZE,\n",
    "        chunk_overlap=settings.CHUNK_OVERLAP\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Assign a unique ID to each chunk to enable resumability and avoid duplicates\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"id\"] = create_chunk_id(chunk, i)\n",
    "\n",
    "    logging.info(f\"‚úÖ Prepared a total of {len(chunks)} chunks to be processed.\")\n",
    "\n",
    "    logging.info(f\"üß† Initializing embedding model: {settings.EMBEDDING_MODEL_NAME} (running on GPU)\")\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=settings.EMBEDDING_MODEL_NAME,\n",
    "        model_kwargs={'device': 'cuda'}, # Leverage the GPU\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "\n",
    "    db = Chroma(\n",
    "        persist_directory=str(settings.DB_PATH),\n",
    "        embedding_function=embedding_model\n",
    "    )\n",
    "\n",
    "    logging.info(\"üîç Checking for already indexed documents in the database...\")\n",
    "    existing_ids = set(db.get(include=[])['ids'])\n",
    "    logging.info(f\"‚úÖ Found {len(existing_ids)} existing chunks in the database.\")\n",
    "\n",
    "    chunks_to_index = [chunk for chunk in chunks if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "    if not chunks_to_index:\n",
    "        logging.info(\"üéâ All documents are already indexed! Nothing to do.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"‚è≥ {len(chunks_to_index)} chunks remaining to be indexed.\")\n",
    "\n",
    "    logging.info(f\"üíæ Adding new chunks to the vector database...\")\n",
    "    batch_size = 128 # A larger batch size is efficient on GPUs\n",
    "\n",
    "    for i in tqdm(range(0, len(chunks_to_index), batch_size), desc=\"Indexing (GPU)\"):\n",
    "        batch = chunks_to_index[i:i + batch_size]\n",
    "        batch_ids = [chunk.metadata[\"id\"] for chunk in batch]\n",
    "        # Using add_documents is an efficient way to add a batch with IDs\n",
    "        db.add_documents(documents=batch, ids=batch_ids)\n",
    "\n",
    "    # .persist() is no longer needed with the new langchain-chroma package.\n",
    "    # The database saves automatically when initialized with a persist_directory.\n",
    "    logging.info(\"üéâ Indexing process completed successfully!\")\n",
    "\n",
    "\n",
    "# --- Part 5: Run the Main Function ---\n",
    "run_indexing()"
   ],
   "id": "b66e5ca9e3dfbfb2"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
