# scripts/build_index.py
"""
Standalone script for building or updating the ChromaDB vector database.

This script reads documents from a specified .jsonl file, processes them into
chunks, generates vector embeddings using a sentence-transformer model, and
ingests them into a persistent ChromaDB store.

It supports incremental updates (resumability) by checking for existing documents
and provides command-line arguments for flexible execution, including processing
subsets of data for testing.

Usage:
    - For a full build using default settings:
      $ python scripts/build_index.py

    - To create a separate test database from the first 50 documents:
      $ python scripts/build_index.py --db-path vector_db_temp --stop 50

    - To process a specific batch of documents:
      $ python scripts/build_index.py --start 1000 --stop 2000
"""

import argparse
import hashlib
import logging
import sys
from pathlib import Path
from typing import Optional

# Ensure the 'src' directory is in the Python path to allow for absolute imports.
project_root = Path(__file__).parent.parent.resolve()
sys.path.append(str(project_root))

from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm

from config import settings
from src.data_processing.loader import load_and_prepare_documents

# Configure logging to provide informative output during the script's execution.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def create_chunk_id(chunk: Document, chunk_index: int) -> str:
    """
    Creates a unique and deterministic ID for a document chunk.

    The ID is generated by creating an SHA-256 hash of the source URL,
    the chunk's content, and its index. This ensures that even identical
    chunks from different documents or positions have unique identifiers.

    Args:
        chunk (Document): The document chunk to ID.
        chunk_index (int): The sequential index of the chunk.

    Returns:
        str: A unique hexadecimal ID for the chunk.
    """
    unique_string = f"{chunk.metadata.get('source', '')}{chunk.page_content}{chunk_index}"
    return hashlib.sha256(unique_string.encode()).hexdigest()


def run_indexing(db_path: Path, start: int = 0, stop: Optional[int] = None):
    """
    The main function to build and populate the vector database.

    Args:
        db_path (Path): The file path to the directory for the ChromaDB instance.
        start (int): The starting line index to process from the input file.
        stop (Optional[int]): The ending line index to process from the input file.
    """
    logging.info(f"🚀 Starting indexing process. Database will be stored at: '{db_path}'")

    # Step 1: Load and prepare documents within the specified range
    documents = load_and_prepare_documents(settings.DATA_PATH, start=start, stop=stop)
    if not documents:
        logging.warning("No documents found in the specified range. Exiting.")
        return

    # Step 2: Split documents into chunks and assign unique IDs
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.CHUNK_SIZE,
        chunk_overlap=settings.CHUNK_OVERLAP
    )
    chunks = text_splitter.split_documents(documents)
    for i, chunk in enumerate(chunks):
        chunk.metadata["id"] = create_chunk_id(chunk, i)
    logging.info(f"✅ Prepared {len(chunks)} chunks for processing.")

    # Step 3: Initialize the embedding model and the vector database client
    embedding_model = HuggingFaceEmbeddings(
        model_name=settings.EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cpu'},  # Explicitly use CPU for local indexing
        encode_kwargs={'normalize_embeddings': True}
    )
    db = Chroma(
        persist_directory=str(db_path),
        embedding_function=embedding_model
    )

    # Step 4: Filter out chunks that are already present in the database
    logging.info("🔍 Checking for already indexed chunks...")
    existing_ids = set(db.get(include=[])['ids'])
    logging.info(f"✅ Found {len(existing_ids)} existing chunks in the database.")
    chunks_to_index = [chunk for chunk in chunks if chunk.metadata["id"] not in existing_ids]

    if not chunks_to_index:
        logging.info("🎉 All documents are already indexed! Nothing to do.")
        return

    logging.info(f"⏳ {len(chunks_to_index)} chunks remaining to be indexed.")

    # Step 5: Index new chunks in batches
    batch_size = 32  # Smaller batch size is generally better for CPU processing
    for i in tqdm(range(0, len(chunks_to_index), batch_size), desc="Indexing Batches (CPU)"):
        batch = chunks_to_index[i:i + batch_size]
        batch_ids = [chunk.metadata["id"] for chunk in batch]
        db.add_documents(documents=batch, ids=batch_ids)

    # Note: .persist() is no longer needed with langchain-chroma's new API.
    # Saving is handled automatically when a persist_directory is provided.
    logging.info(f"🎉 Indexing process completed successfully! Database saved in '{db_path}'.")


def setup_arg_parser() -> argparse.ArgumentParser:
    """
    Sets up the command-line argument parser for the script.

    Returns:
        argparse.ArgumentParser: The configured argument parser.
    """
    parser = argparse.ArgumentParser(
        description="Build or update the vector database from a .jsonl file."
    )
    parser.add_argument(
        '--db-path',
        type=str,
        default=str(settings.DB_PATH),
        help=f"Path to the vector database directory. Default: '{settings.DB_PATH}'."
    )
    parser.add_argument(
        '--start',
        type=int,
        default=0,
        help="The starting line in the .jsonl file to process."
    )
    parser.add_argument(
        '--stop',
        type=int,
        default=None,
        help="The ending line (exclusive) in the .jsonl file to process."
    )
    return parser


if __name__ == '__main__':
    arg_parser = setup_arg_parser()
    args = arg_parser.parse_args()
    run_indexing(db_path=Path(args.db_path), start=args.start, stop=args.stop)
